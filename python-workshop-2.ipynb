{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSI Python Workshop, Part 2\n",
    "\n",
    "Information about the U.S. Congress is available in analysis-friendly formats (CSV, JSON, etc) from several different online sources. One example is the [ProPublica Congress API][propublica-api]. On the other hand, information about the California State Legislature is less accessible.\n",
    "\n",
    "Today we'll scrape the web to get biographical details about members of the California legislature, and then use this data to make a few visualizations.\n",
    "\n",
    "[propublica-api]: https://propublica.github.io/congress-api-docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The California Assembly and Senate official web sites have member names and addresses, but not much else. Wikipedia is a conviently centralized source of information about public office holders. We'll start by scraping the [California State Legislature 2017-18 page][ca-legislature]\n",
    " at:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/California_State_Legislature,_2017%E2%80%9318_session\n",
    "```\n",
    "\n",
    "[ca-legislature]: https://en.wikipedia.org/wiki/California_State_Legislature,_2017%E2%80%9318_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Downloading The Page\n",
    "\n",
    "The first step is to download the page. We'll use __requests__ for this. __Requests__ is a Python package for sending HTTP requests--the same requests your web browser sends to download web pages.\n",
    "\n",
    "There are a few [different kinds of HTTP requests][http-requests]. We're only going to send __GET__ requests, which are for getting web pages or other data from a server.\n",
    "\n",
    "The server's response comes with an [HTTP status code][http-status] to indicate whether anything went wrong.\n",
    "\n",
    "[http-status]: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "[http-requests]: https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting Links\n",
    "\n",
    "Now that we have the page, we need to extract a link for each of the 40 senators. Like most web pages, the page is written in HTML.\n",
    "\n",
    "HTML uses \"tags\" to mark how text should be structured and formatted. Tags are written in angle brackets. Most tags come in pairs that surround the text they apply to. Closing tags begin with `</` rather than `<`. For instance, the `<b>` tag marks text as bold:\n",
    "```html\n",
    "This is regular text. <b>This is bold text.</b> This is more regular text.\n",
    "```\n",
    "Tags may include other tags as sub-elements.\n",
    "\n",
    "Start tags can also include \"attributes\", which are additional information written after the tag name. As an example, the `<a>` tag marks a hyperlink and usually includes an `href` attribute with the URL to link to:\n",
    "```html\n",
    "<a href=\"http://www.google.com/\" id=\"my_link\">Google</a>\n",
    "```\n",
    "Attributes are space-separated; the tag above has two attributes.\n",
    "\n",
    "__lxml__ is a Python package for parsing HTML (and XML) documents. We can use __lxml__ to extract text from tags and attributes in the page we downloaded.\n",
    "\n",
    "We'll use CSS selectors to tell __lxml__ which tags we want. You can learn more about CSS selectors through [this interactive tutorial][diner].\n",
    "\n",
    "[diner]: http://flukeout.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "\n",
    "Get links to the Wikipedia pages for each of California's 80 assembly members."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Downloading And Extracting Biographical Details\n",
    "\n",
    "Extracting the biographical details for one senator follows the same procedure as the previous two steps. First we download the page with __requests__, and then parse the HTML with __lxml__.\n",
    "\n",
    "The data are in a table again, and since we want the table's content rather than its attributes, it makes sense to use the `read_html()` function from __pandas__. This function uses __lxml__ behind the scenes to convert all of the tables in a page into data frames.\n",
    "\n",
    "Finally, since we need to apply these operations to the pages for all 40 senators, we'll call the code from a loop over the senator URLs. \n",
    "\n",
    "Loop bodies are good candidates for function definitions, so we should move the code into a function definition before writing the loop. This is especially important for Python, where loops can be written as a comprehensions. Comprehensions use less memory and are sometimes faster than regular loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "\n",
    "Modify `scrape_bio()` so that it also retrieves \"Alma mater\" and \"Residence\". If you finish early, try retrieving other details as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Scrubbing The Data\n",
    "\n",
    "Some of the columns in the extracted data frame contain multiple biographical details. For instance, each senator's age is embedded in the `born` column.\n",
    "\n",
    "Let's extract the ages with the text processing methods in __pandas__. With these we can operate on entire columns at once, which is faster than writing a loop (akin to vectorization in _R_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Visualizing The Data\n",
    "\n",
    "__Bokeh__ is a Python package for creating semi-interactive visualizations. The package is [well-documentated][bokeh-docs] and all of its plotting functions expect [tidy data][tidy] as input.\n",
    "\n",
    "[bokeh-docs]: http://bokeh.pydata.org/en/latest/docs/user_guide.html\n",
    "[tidy]: http://vita.had.co.nz/papers/tidy-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "\n",
    "Extract each senator's birth state from the data frame. Make a bar plot of that shows the number of senators born in each place."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
